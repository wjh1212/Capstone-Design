# 블록도 
![캡쳐](https://user-images.githubusercontent.com/103232862/234587772-39a4b1bd-f045-4a7b-a1e1-99c2f96eaa21.PNG)

# 진행사항 - part 1

- 하기의 내용은 edge impulse를 통하여 학습된 결과 및 rp2040에 내장된 마이크와 arduino ide에서 제공하는 샘플 코드를 이용하여 음성 인식률을 확인해보는 과정이다. 

(1) .wav 확장자의 sound sample 저장. 
- 예시) yes, no, stop  -> background sound는 하기에 설명함.

![image](https://user-images.githubusercontent.com/103232862/234507881-1fa028d7-8f11-4862-b89b-a5f3a588de7b.png)


(2) 저장된 sample 파일들을 edge impulse에서 학습.

![image](https://user-images.githubusercontent.com/103232862/234510171-fcb7fc52-7665-4910-81d9-0cb81abb60e4.png)

![image](https://user-images.githubusercontent.com/103232862/234509130-454287f5-a66b-4690-89f9-c6025f4866aa.png)

(3) 학습시킨 음성 샘플을 arduino ide 라이브러리 .zip 파일로 추출함.

![image](https://user-images.githubusercontent.com/103232862/234511134-4d1a7701-1faf-479a-89df-ef3f21e461ac.png)

(4) .zip 파일 라이브러리를 추가한 후 rp2040_microphone_continouse.ino 샘플 코드를 수행.

![image](https://user-images.githubusercontent.com/103232862/234511382-f6aba3a9-2789-4e3c-9135-3ec8359b1922.png)
![image](https://user-images.githubusercontent.com/103232862/234511428-67f3ca23-c2f9-42b7-9edb-c5da95df8b5e.png)

(5) 마이크로 음성을 말하면 해당 text의 수치가 증가함.

![ezgif-4-f00d15dd30](https://user-images.githubusercontent.com/103232862/234505898-b5826c83-5435-48a3-a167-9f646d844ce0.gif)

![image](https://user-images.githubusercontent.com/103232862/234506416-6e0bd6f0-9b48-4089-8a45-13ac3500a165.png)

ex) no라고 했을 경우 no의 수치가 증가함.

# 향후 계획

- 명령어가 입력될 경우 해당 label 번호를 mqtt 통신을 통해 raspberry pi로 전송 후 node-red를 통해 보여줌.

![image](https://user-images.githubusercontent.com/103232862/234513672-910e2bd9-9819-4039-b980-88035561e482.png)


- 상기의 사진은 serial monitor에서 잡음이나 침묵 부분이 비어있다고 경고하는 메세지가 출력된 장면.

![image](https://user-images.githubusercontent.com/103232862/234513988-bcde3517-7ac2-4c25-9c63-356bda607167.png)

- 해당 오류를 해결하기 위해 잡음, 무음 sample 파일을 구한 뒤 추가할 계획.

![image](https://user-images.githubusercontent.com/103232862/234514435-f973184c-9ddb-4774-ab1a-747ba5c60cc3.png)
![image](https://user-images.githubusercontent.com/103232862/234514465-66ff81f3-aa7a-4391-ba9c-b575cfe81f02.png)
![image](https://user-images.githubusercontent.com/103232862/234514573-e9459db1-7c6c-4304-820d-91d39adc064a.png)

- 상기의 사진들은 잡음, 무음 sample 파일을 추가하는 과정.

# 진행사항 - part 2

하기의 내용은 인공지능을 활용하여 얼굴은 학습시키고 인식하는 과정이다.

faceCascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_default.xml')
분류기를 사용하여 얼굴부분만을 추출한다.



face_id = input('\n enter user id end press <return> ==> ')
print("\n [INFO] Initializing face capture. Look the camera and wait ...")
사람들의 각 얼굴마다 번호를 지정해 준후 데이터셋 폴더에 저장한다.
  
  
  
if cv2.waitKey(1) > 0 : break #키 입력이 있을 때 반복문 종료
elif count >= 100 : break #100 face sample
100장의 얼굴 데이터 사진을 추출하고 종류시킨다.
  
  
  
  
텐서플로우의 Local-Binary-Pattern 알고리즘 함수를 사용하여 얼굴을 학습시킨다.
path = 'dataset'
recognizer = cv2.face.LBPHFaceRecognizer_create()
데이터셋에 있는 폴더에 있는 사진들을 보고 학습한다.
  
  
  
  
  
  for(x,y,w,h) in faces:
        cv2.rectangle(img, (x,y), (x+w,y+h), (0,255,0),2)
        id, confidence = recognizer.predict(gray[y:y+h, x:x+w])

        if confidence < 55 :
            id = names[id]
        else:
            id = "unknown"
        
        confidence = "  {0}%".format(round(100-confidence))

        cv2.putText(img,str(id), (x+5,y-5),font,1,(255,255,255),2)
        cv2.putText(img,str(confidence), (x+5,y+h-5),font,1,(255,255,0),1)
 그 후 학습시킨 내용을 바탕으로 인공지능을 실행한다.
  
  
  
                          
                          
# 진행사항 - part 3
  
fcm을 이용하여 데이터 메세지를 보내면 앱이 push를 받고 실행이 된다, push를 받고 실행이 된 경우에 사전에 지정이된 번호로 전화를 걸게 수행한다. 
                          
어플리케이션은 안드로이드 스튜디오를 이용하여 개발하였다. 


![안드로이드 앱](https://user-images.githubusercontent.com/103232862/234572735-57a560af-71ee-487d-b13b-44c82416d54b.jpg)

           
                          
## 향후계획
                          
part1, part2의 선행기능들이 완성되면 Push Message 송신 기능을 추가함으로써 본 어플리케이션을 통해 사전에 지정된 전화번호로 자동으로 전화를 발신하는 기능을 수행할것이다.                          
                          
